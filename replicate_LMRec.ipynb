{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZYI3YutL2Ji0NO/rl6yh4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krooner/til/blob/main/replicate_LMRec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "68CijRZXiuvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip list | grep kaggle\n",
        "# !pip list | grep transformers\n",
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "o8brkJsZjkwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "Kaggle"
      ],
      "metadata": {
        "id": "iSkXDKQKtpxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /root/.config/kaggle\n",
        "!cp '/gdrive/MyDrive/datastore/kaggle.json' /root/.config/kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "DubAveq9jTQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download carrie1/ecommerce-data && unzip ecommerce-data.zip"
      ],
      "metadata": {
        "id": "Ifkd6QOwj4b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "qmAybTd7kkm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = pd.read_csv(\"/content/data.csv\", encoding='latin-1')\n",
        "display(dataframe.shape)\n",
        "display(dataframe.head())"
      ],
      "metadata": {
        "id": "O_Z1DbXqkoTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = dataframe.loc[~dataframe.CustomerID.isnull()].astype({'CustomerID': int})\n",
        "\n",
        "display(dataframe.shape)\n",
        "display(dataframe.head())"
      ],
      "metadata": {
        "id": "8tzeNWBAlIz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataframe.CustomerID.unique().shape # 4372\n",
        "user_df = dataframe.sort_values('InvoiceDate')\n",
        "user_df = dataframe.groupby('CustomerID').agg({'Description': lambda x: \"->\".join(x), 'Quantity': len}).reset_index()\n",
        "user_df = user_df.loc[user_df.Quantity>1]\n",
        "\n",
        "user_df['user_sequence'] = user_df.Description.apply(lambda x: \"->\".join(x.split(\"->\")[:-1]))\n",
        "user_df['user_item'] = user_df.Description.apply(lambda x: x.split(\"->\")[-1])\n",
        "user_df.head()\n",
        "# user_df.CustomerID.unique().shape # 4293"
      ],
      "metadata": {
        "id": "oH0cia_Hrw7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_pandas(user_df)"
      ],
      "metadata": {
        "id": "nauCFP4Q2xw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer\n",
        "from transformers import GPT2TokenizerFast\n",
        "model_tag = 'skt/kogpt2-base-v2'\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_tag)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_tag)"
      ],
      "metadata": {
        "id": "N_PdW3rDOkEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer.encode('<s></s>')\n",
        "print(encoded)\n",
        "print(tokenizer.convert_ids_to_tokens(encoded))\n",
        "decoded = tokenizer.decode(encoded)\n",
        "print(decoded)"
      ],
      "metadata": {
        "id": "AQI4rMtcexLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer.encode('<|endoftext|><|endoftext|>')\n",
        "print(encoded)\n",
        "print(tokenizer.convert_ids_to_tokens(encoded))\n",
        "decoded = tokenizer.decode(encoded)\n",
        "print(decoded)"
      ],
      "metadata": {
        "id": "LEll2J4vfR_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "tokenizer.bos_token = '<s>'\n",
        "tokenizer.bos_token_id = 0\n",
        "\n",
        "tokenizer.eos_token = '</s>'\n",
        "tokenizer.eos_token_id = 1\n",
        "\n",
        "tokenizer.pad_token = '<pad>'\n",
        "tokenizer.pad_token_id = 3\n",
        "\n",
        "tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"<s> $0 </s>\",\n",
        "    special_tokens=[(\"<s>\", tokenizer.bos_token_id), (\"</s>\", tokenizer.eos_token_id)]\n",
        ")"
      ],
      "metadata": {
        "id": "Kf9cVL-zdDhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(inputs):\n",
        "  tokenized_data = tokenizer(inputs['Description'], padding=True, truncation=True, max_length=32)\n",
        "  return tokenized_data\n",
        "\n",
        "dataset_mapped = dataset.map(\n",
        "    tokenize_data,\n",
        "    batched=True\n",
        ")"
      ],
      "metadata": {
        "id": "p32-Sd8x2_9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_mapped"
      ],
      "metadata": {
        "id": "WIiGNhIO3gEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens(range(10))"
      ],
      "metadata": {
        "id": "l_PDsw2gkcR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample_input_ids = dataset_mapped[0]['input_ids']\n",
        "sample_texts = [\n",
        "    \"Hello World\",\n",
        "    \"We are the world\"\n",
        "]\n",
        "tokenize_text = tokenizer(sample_texts, padding=True, truncation=True, max_length=16, return_tensors='pt')\n",
        "\n",
        "eos_token_pos = (tokenize_text['input_ids'] == tokenizer.eos_token_id).nonzero()\n",
        "display(tokenize_text['input_ids'])\n",
        "display(tokenize_text['input_ids'] == tokenizer.eos_token_id)\n",
        "display(eos_token_pos[:, -1])"
      ],
      "metadata": {
        "id": "0LKu4gv6cZTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(tokenizer.bos_token, tokenizer.bos_token_id)\n",
        "display(tokenizer.eos_token, tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "GzANdQS7hmsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling"
      ],
      "metadata": {
        "id": "3gp_lfpftkJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import AutoModelForCausalLM\n",
        "# from transformers import AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_tag)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_tag)\n",
        "\n",
        "display(model)\n",
        "# display(tokenizer)"
      ],
      "metadata": {
        "id": "k7DDEEzGsIwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import numpy as np\n",
        "\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "class LMRecModel(nn.Module):\n",
        "  def __init__(self, model_name=\"skt/kogpt2-base-v2\"):\n",
        "    super().__init__()\n",
        "\n",
        "    self.language_model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True)\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    self.n_dim = self.language_model.config.n_embd\n",
        "\n",
        "    self.coef = 1e-3\n",
        "\n",
        "    self.user_linear = nn.Linear(self.n_dim, self.n_dim)\n",
        "    self.item_linear = nn.Linear(self.n_dim, self.n_dim)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    self.loss_fct = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    self.tokenizer.bos_token = '<s>'\n",
        "    self.tokenizer.bos_token_id = 0\n",
        "\n",
        "    self.tokenizer.eos_token = '</s>'\n",
        "    self.tokenizer.eos_token_id = 1\n",
        "\n",
        "    self.tokenizer.pad_token = '<pad>'\n",
        "    self.tokenizer.pad_token_id = 3\n",
        "\n",
        "    self.tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
        "        single=\"<s> $0 </s>\",\n",
        "        special_tokens=[(\"<s>\", self.tokenizer.bos_token_id), (\"</s>\", self.tokenizer.eos_token_id)]\n",
        "    )\n",
        "\n",
        "  def forward(self, user_input_ids, user_attention_mask, item_input_ids, item_attention_mask, labels=None):\n",
        "    user_outputs = self.language_model(\n",
        "        input_ids=user_input_ids,\n",
        "        attention_mask=user_attention_mask,\n",
        "        labels=user_input_ids.clone()\n",
        "    )\n",
        "    causal_lm_loss = user_outputs.loss\n",
        "\n",
        "    user_eos_indices = (user_input_ids == self.tokenizer.eos_token_id).nonzero(as_tuple=True)[-1]\n",
        "    item_eos_indices = (item_input_ids == self.tokenizer.eos_token_id).nonzero(as_tuple=True)[-1]\n",
        "\n",
        "    last_hidden_state_user = user_outputs.hidden_states[-1]\n",
        "    # user_embedding = user_outputs.hidden_states[-1][:, -1, :] # (batch_size, max_length, n_dim) -> (batch_size, n_dim)\n",
        "\n",
        "    # user_embedding = torch.Tensor(\n",
        "    #     [last_hidden_state_user[i, eos_indices[i], :] for i in range(len(user_input_ids))]\n",
        "    # ).reshape((-1, self.n_dim)) # (batch_size, max_length, n_dim) -> (batch_size, n_dim)\n",
        "\n",
        "    user_embedding = last_hidden_state_user[torch.arange(len(user_input_ids)), user_eos_indices]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      item_outputs = self.language_model(\n",
        "          input_ids=item_input_ids,\n",
        "          attention_mask=item_attention_mask,\n",
        "      )\n",
        "\n",
        "    last_hidden_state_item = item_outputs.hidden_states[-1]\n",
        "    # item_embedding = item_outputs.hidden_states[-1][:, -1, :] # (batch_size, max_length, n_dim) -> (batch_size, n_dim)\n",
        "\n",
        "    # item_embedding = torch.Tensor(\n",
        "    #     [last_hidden_state_item[i, eos_indices[i], :] for i in range(len(item_input_ids))]\n",
        "    # ).reshape((-1, self.n_dim))\n",
        "\n",
        "    item_embedding = last_hidden_state_item[torch.arange(len(item_input_ids)), item_eos_indices]\n",
        "\n",
        "    user_hidden = self.user_linear(user_embedding)\n",
        "    item_hidden = self.item_linear(item_embedding)\n",
        "\n",
        "    user_item_dot_product = (user_hidden * item_hidden).sum(dim=1)\n",
        "\n",
        "    user_item_prob = self.sigmoid(user_item_dot_product)\n",
        "\n",
        "    rec_loss = self.loss_fct(user_item_prob, labels.float())\n",
        "\n",
        "    total_loss = causal_lm_loss + self.coef * rec_loss\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "  # TODO\n",
        "  def encode_user(self, user_input_ids, user_attention_mask):\n",
        "    self.language_model.eval()\n",
        "    with torch.no_grad():\n",
        "      user_outputs = self.language_model(\n",
        "          input_ids=user_input_ids,\n",
        "          attention_mask=user_attention_mask\n",
        "      )\n",
        "\n",
        "    user_eos_indices = (user_input_ids == self.tokenizer.eos_token_id).nonzero(as_tuple=True)[-1]\n",
        "\n",
        "    last_hidden_state_user = user_outputs.hidden_states[-1].cpu().detach()\n",
        "\n",
        "    user_embedding = last_hidden_state_user[torch.arange(len(user_input_ids)), user_eos_indices]\n",
        "\n",
        "    user_hidden = self.user_linear(user_embedding)\n",
        "\n",
        "    return user_hidden\n",
        "\n",
        "  # TODO\n",
        "  def encode_item(self, item_input_ids, item_attention_mask):\n",
        "    self.language_model.eval()\n",
        "    with torch.no_grad():\n",
        "      item_outputs = self.language_model(\n",
        "          input_ids=item_input_ids,\n",
        "          attention_mask=item_attention_mask\n",
        "      )\n",
        "\n",
        "    item_eos_indices = (item_input_ids == self.tokenizer.eos_token_id).nonzero(as_tuple=True)[-1]\n",
        "\n",
        "    last_hidden_state_item = item_outputs.hidden_states[-1].cpu().detach()\n",
        "\n",
        "    item_embedding = last_hidden_state_item[torch.arange(len(item_input_ids)), item_eos_indices]\n",
        "\n",
        "    item_hidden = self.item_linear(user_embedding)\n",
        "\n",
        "    return item_hidden\n",
        "\n",
        "  def get_user_item_probability(self, user_hidden, item_hidden):\n",
        "\n",
        "    return self.sigmoid(torch.matmul(user_hidden, item_hidden.t()))\n",
        "\n",
        "    # user_item_dot_product = (user_hidden * item_hidden).sum(dim=1)\n",
        "    # user_item_prob = self.sigmoid(user_item_dot_product)\n",
        "    # return user_item_prob.detach().numpy()"
      ],
      "metadata": {
        "id": "mbJXufAatBGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LMRecModel()"
      ],
      "metadata": {
        "id": "dp4jyNmgyR1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_seq = [\"안녕하세요. 고객센터입니다.\", \"Hello World\"]\n",
        "item_seq = [\"세탁기 소음 문제 해결 방법 안내\", \"How are you\"]\n",
        "\n",
        "user_inputs = tokenizer(user_seq, padding=True, truncation=True, max_length=32, return_tensors='pt')\n",
        "item_inputs = tokenizer(item_seq, padding=True, truncation=True, max_length=32, return_tensors='pt')\n",
        "\n",
        "displays\n",
        "\n",
        "lm_labels = user_inputs[\"input_ids\"].clone()\n",
        "lm_labels"
      ],
      "metadata": {
        "id": "qApog28c-Gtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_labels[lm_labels==tokenizer.pad_token_id] = -100\n",
        "display(lm_labels)\n",
        "\n",
        "labels = torch.tensor([1, 0])\n",
        "display(labels)"
      ],
      "metadata": {
        "id": "KNzTLbeIgIuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(\n",
        "    user_input_ids=user_inputs['input_ids'],\n",
        "    user_attention_mask=user_inputs['attention_mask'],\n",
        "    item_input_ids=item_inputs['input_ids'],\n",
        "    item_attention_mask=item_inputs['attention_mask'],\n",
        "    labels=labels,\n",
        "  )\n",
        "outputs"
      ],
      "metadata": {
        "id": "kh9Plj2WgVaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_embedding = model.encode_user(\n",
        "    user_input_ids=user_inputs['input_ids'],\n",
        "    user_attention_mask=user_inputs['attention_mask'],\n",
        "  )\n",
        "\n",
        "item_embedding = model.encode_item(\n",
        "    item_input_ids=item_inputs['input_ids'],\n",
        "    item_attention_mask=item_inputs['attention_mask'],\n",
        "  )\n",
        "user_embedding.shape, item_embedding.shape"
      ],
      "metadata": {
        "id": "FDIi7fdHhPsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_item_prob = model.get_user_item_probability(user_embedding, item_embedding)\n",
        "user_item_prob"
      ],
      "metadata": {
        "id": "-jwEPuhQ2F6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sv8zSwA8BwfJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}